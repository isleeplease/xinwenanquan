import pandas as pd
import numpy as np
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re
import jieba
import matplotlib.font_manager as fm
from collections import Counter
import os


# è‡ªåŠ¨è¯†åˆ«ä¸­æ–‡å­—ä½“è·¯å¾„
def find_chinese_font():
    """æŸ¥æ‰¾ç³»ç»Ÿä¸­çš„ä¸­æ–‡å­—ä½“"""
    font_path = None
    # å¸¸è§å­—ä½“è·¯å¾„
    font_candidates = [
        'simhei.ttf', 'simsun.ttc', 'msyh.ttc', 'msyhbd.ttc',
        'STHeiti Medium.ttc', 'PingFang.ttc', 'NotoSansCJKsc-Regular.otf'
    ]

    # ç³»ç»Ÿå­—ä½“ç›®å½•
    font_dirs = [
        'C:/Windows/Fonts/',
        '/Library/Fonts/',
        '/usr/share/fonts/truetype/'
    ]

    # å°è¯•æŸ¥æ‰¾å­˜åœ¨çš„å­—ä½“
    for font in font_candidates:
        for directory in font_dirs:
            path = os.path.join(directory, font)
            if os.path.exists(path):
                return path

    # å¦‚æœæœªæ‰¾åˆ°ï¼Œå°è¯•è¿”å›matplotlibå·²åŠ è½½çš„å­—ä½“
    try:
        for font in fm.findSystemFonts():
            if any(name in font.lower() for name in ['simhei', 'simsun', 'msyh', 'pingfang']):
                return font
    except:
        pass

    return None


# å¢å¼ºçš„æ•°æ®åŠ è½½å‡½æ•°
def load_comments_data(file_path):
    """åŠ è½½è¯„è®ºæ•°æ®ï¼Œå¤„ç†å¸¸è§å¼‚å¸¸"""
    print(f"åŠ è½½æ–‡ä»¶: {file_path}")
    try:
        # å°è¯•è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path)
        print(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶: {len(df)}æ¡è®°å½•")

        # æ£€æŸ¥åˆ—å
        print(f"æ–‡ä»¶åŒ…å«çš„åˆ—: {list(df.columns)}")

        # åˆ—åå¤„ç† (å…¼å®¹ä¸åŒæ ¼å¼çš„åˆ—å)
        rename_map = {}

        # æ–‡æœ¬åˆ—
        text_cols = ['text', 'cleaned_text', 'è¯„è®ºå†…å®¹', 'è¯„è®º', 'content']
        for col in text_cols:
            if col in df.columns:
                rename_map[col] = 'text'
                break
        else:  # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æœ¬åˆ—
            raise ValueError("é”™è¯¯: æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è¯„è®ºæ–‡æœ¬åˆ—")

        # ç‚¹èµæ•°åˆ—
        like_cols = ['like_count', 'likes', 'ç‚¹èµæ•°', 'ç‚¹èµ', 'favorites']
        for col in like_cols:
            if col in df.columns:
                rename_map[col] = 'like_count'
                break

        # æ—¥æœŸåˆ—
        date_cols = ['published_at', 'date', 'æ—¶é—´', 'timestamp', 'è¯„è®ºæ—¶é—´']
        for col in date_cols:
            if col in df.columns:
                rename_map[col] = 'published_at'
                break

        # æƒ…æ„Ÿåˆ—
        sentiment_cols = ['sentiment', 'æƒ…æ„Ÿ', 'æƒ…æ„Ÿåˆ†æ', 'emotion']
        for col in sentiment_cols:
            if col in df.columns:
                rename_map[col] = 'sentiment'
                break

        # åº”ç”¨åˆ—é‡å‘½å
        df = df.rename(columns=rename_map)

        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        required_cols = ['text']
        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"é”™è¯¯: å¿…éœ€çš„åˆ— '{col}' ç¼ºå¤±")

        # å¡«å……ç¼ºå¤±å€¼
        if 'like_count' not in df.columns:
            print("âš ï¸ è­¦å‘Š: ç‚¹èµæ•°åˆ—ç¼ºå¤±ï¼Œæ‰€æœ‰è¯„è®ºç‚¹èµæ•°è®¾ä¸º1")
            df['like_count'] = 1

        if 'published_at' not in df.columns:
            print("âš ï¸ è­¦å‘Š: æ—¶é—´æˆ³åˆ—ç¼ºå¤±ï¼Œæ‰€æœ‰è¯„è®ºè®¾ä¸ºå½“å‰æ—¶é—´")
            df['published_at'] = pd.Timestamp.now()

        if 'sentiment' not in df.columns:
            print("âš ï¸ è­¦å‘Š: æƒ…æ„Ÿåˆ†æåˆ—ç¼ºå¤±ï¼Œæ‰€æœ‰è¯„è®ºè®¾ä¸º'neutral'")
            df['sentiment'] = 'neutral'

        # æ‰“å°ä¿¡æ¯
        print(f"å¤„ç†ååŒ…å«çš„åˆ—: {list(df.columns)}")
        if 'like_count' in df.columns:
            print(
                f"ç‚¹èµæ•°ç»Ÿè®¡ (min/max/avg): {df['like_count'].min():,}/{df['like_count'].max():,}/{df['like_count'].mean():.1f}")

        return df

    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")
        return pd.DataFrame()


# å¤šè¯­è¨€æ–‡æœ¬å¤„ç†
def process_multilingual_text(text):
    """å¤„ç†å¤šè¯­è¨€æ··åˆæ–‡æœ¬"""
    if not isinstance(text, str):
        return []

    # åŸºæœ¬æ¸…ç†
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)  # ä¿ç•™ä¸­æ–‡æ–‡å­—å­—ç¬¦
    text = text.lower()

    # è¯­è¨€æ£€æµ‹
    has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text)
    has_english = bool(re.search(r'[a-z]', text))

    # æ··åˆè¯­è¨€å¤„ç†
    words = []

    # ä¸­æ–‡åˆ†è¯ (ä½¿ç”¨jieba)
    if has_chinese:
        try:
            # ä¸­æ–‡æ–‡æœ¬åˆ†
            ch_words = [word for word in jieba.cut(text) if len(word) > 1]
            words.extend(ch_words)
        except:
            # å¦‚æœjiebaå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•åˆ†è¯
            ch_words = re.findall(r'[\u4e00-\u9fff]{2,}', text)
            words.extend(ch_words)

    # è‹±æ–‡åˆ†è¯
    if has_english:
        en_words = re.findall(r'[a-z]{3,}', text)  # ä»…ä¿ç•™3å­—ç¬¦ä»¥ä¸Šçš„è‹±æ–‡å•è¯
        words.extend(en_words)

    return words


def get_stopwords():
    """è·å–ä¸­è‹±æ–‡åœç”¨è¯åˆ—è¡¨"""
    # ä¸­æ–‡åœç”¨è¯
    chinese_stopwords = set([
        'çš„', 'äº†', 'åœ¨', 'å’Œ', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£', 'å°±', 'ä¸', 'ä¹Ÿ', 'æœ‰', 'æ²¡æœ‰',
        'æ²¡', 'å•Š', 'å“¦', 'å—¯', 'å‘€', 'å§', 'å‘¢', 'å—', 'å•¦', 'å“‡', 'å“ˆ', 'å”‰', 'å“Ÿ', 'å‘µ', 'å˜¿', 'å“¼', 'è‡ªå·±',
        'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å¯ä»¥', 'å¯èƒ½', 'å¯æ˜¯', 'è®©', 'æŠŠ', 'è¢«', 'ç»™', 'å¯¹', 'å‘', 'è·Ÿ', 'å’Œ',
        'ä¸', 'åŒ', 'äº†', 'ç€', 'è¿‡', 'å¾—', 'åœ°', 'çš„', 'å•Š', 'å§', 'å‘¢', 'å—'
    ])

    # è‹±æ–‡åœç”¨è¯ - ç‰¹åˆ«æ·»åŠ äº†ä»£è¯ã€ä»‹è¯å’Œå¸¸ç”¨åŠ¨è¯
    english_stopwords = set([
        'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what', 'which', 'this', 'that',
        'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
        'having', 'do', 'does', 'did', 'doing', 'will', 'would', 'shall', 'should', 'can', 'could', 'may',
        'might', 'must', 'to', 'from', 'in', 'on', 'at', 'by', 'with', 'about', 'against', 'between',
        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'up', 'down', 'out', 'off', 'over',
        'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',
        'any', 'both', 'each', 'few', 'more', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',
        'than', 'too', 'very', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're",
        "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',
        'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their',
        'theirs', 'themselves'
    ])

    # æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦
    numbers = set([str(i) for i in range(0, 100)])
    special_chars = set(['', ' ', '..', '...', '....', '.....', 'rt'])

    # åˆå¹¶æ‰€æœ‰åœç”¨è¯
    return chinese_stopwords | english_stopwords | numbers | special_chars


# ä¸»è¯äº‘ç”Ÿæˆå‡½æ•°
def generate_wordcloud(df, font_path=None):
    """ä»æ•°æ®æ¡†ç”Ÿæˆè¯äº‘"""
    if df.empty:
        print("é”™è¯¯: æ•°æ®æ¡†ä¸ºç©º")
        return

    # è·å–æ‰©å±•çš„åœç”¨è¯åˆ—è¡¨
    stopwords = get_stopwords()

    # å‡†å¤‡æƒé‡è®¡æ•°å™¨
    word_weights = Counter()

    # å¤„ç†æ¯æ¡è¯„è®º
    total = len(df)
    for i, row in df.iterrows():
        text = row.get('text', '')
        if not text:
            continue

        # åˆ†è¯
        words = process_multilingual_text(text)

        # è®¡ç®—æƒé‡ (ä½¿ç”¨ç‚¹èµæ•°çš„å¯¹æ•°ï¼Œé˜²æ­¢æç«¯å€¼ä¸»å¯¼)
        weight = np.log1p(row.get('like_count', 1))

        # æ›´æ–°è¯æƒé‡
        for word in set(words):  # æ¯æ¡è¯„è®ºä¸­æ¯ä¸ªè¯åªè®¡ä¸€æ¬¡
            # æ£€æŸ¥å•è¯æ˜¯å¦åœ¨åœç”¨è¯åˆ—è¡¨ä¸­
            if (len(word) >= 2 and
                    word not in stopwords and
                    not word.isnumeric()):
                word_weights[word] += weight

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆè¯æ±‡
    if not word_weights:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå…³é”®è¯")
        return

    # è·å–æœ€é«˜æƒé‡è¯
    top_words = word_weights.most_common(10)
    print(f"\nğŸ”ğŸ” æœ€é«˜æƒé‡è¯: ")
    for word, weight in top_words:
        print(f"  {word}: {weight:.1f}")

    # è‡ªåŠ¨è·å–å­—ä½“
    if not font_path:
        font_path = find_chinese_font()

    if not font_path:
        print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
        font_path = None
    else:
        print(f"âœ… ä½¿ç”¨å­—ä½“: {os.path.basename(font_path)}")

    # ç”Ÿæˆè¯äº‘
    print("\nğŸ–¼ğŸ–¼ğŸ–¼ï¸ ç”Ÿæˆè¯äº‘ä¸­...")
    wc = WordCloud(
        font_path=font_path,
        width=1200,
        height=800,
        background_color='white',
        max_words=200,
        collocations=False,
        prefer_horizontal=0.8,
        colormap='viridis'
    ).generate_from_frequencies(word_weights)

    # æ˜¾ç¤ºè¯äº‘
    plt.figure(figsize=(15, 10))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"YouTubeè¯„è®ºè¯äº‘ (å…±{total}æ¡è¯„è®º)", fontsize=16)
    plt.tight_layout()
    plt.show()
    print("âœ… è¯äº‘ç”Ÿæˆå®Œæˆ!")


# ä¸»ç¨‹åº
# ä¸»ç¨‹åº
if __name__ == "__main__":
    # æ–‡ä»¶è·¯å¾„
    file_path = "C:/wordcloud/YouTubeè¯„è®º.xlsx"

    # åŠ è½½æ•°æ®
    df = load_comments_data(file_path)

    if not df.empty:
        # ==== æ·»åŠ ç­›é€‰åŠŸèƒ½ ====
        print("\n==== ç­›é€‰é€‰é¡¹ ====")

        # 1. æ—¶é—´ç­›é€‰ (å¦‚æœå­˜åœ¨æ—¶é—´åˆ—)
        if 'published_at' in df.columns:
            # è½¬æ¢ä¸ºæ—¥æœŸç±»å‹
            df['published_at'] = pd.to_datetime(df['published_at'])

            # è·å–æ—¶é—´èŒƒå›´
            min_date = df['published_at'].min().strftime('%Y-%m-%d')
            max_date = df['published_at'].max().strftime('%Y-%m-%d')
            print(f"æ•°æ®æ—¶é—´èŒƒå›´: {min_date} è‡³ {max_date}")

            # ç”¨æˆ·è¾“å…¥æ—¶é—´èŒƒå›´
            start_date = input(f"è¾“å…¥å¼€å§‹æ—¥æœŸ(æ ¼å¼:YYYY-MM-DD, ç•™ç©ºåˆ™ä» {min_date} å¼€å§‹): ") or min_date
            end_date = input(f"è¾“å…¥ç»“æŸæ—¥æœŸ(æ ¼å¼:YYYY-MM-DD, ç•™ç©ºåˆ™åˆ° {max_date} ç»“æŸ): ") or max_date

            # åº”ç”¨ç­›é€‰
            df = df[(df['published_at'] >= start_date) &
                    (df['published_at'] <= end_date)]
            print(f"â° æ—¶é—´ç­›é€‰: {start_date} è‡³ {end_date}, å‰©ä½™ {len(df)} æ¡è¯„è®º")

        # 2. æƒ…æ„Ÿç­›é€‰ (å¦‚æœå­˜åœ¨æƒ…æ„Ÿåˆ—)
        if 'sentiment' in df.columns:
            # è·å–æ‰€æœ‰æƒ…æ„Ÿç±»åˆ«
            sentiments = df['sentiment'].unique()
            print(f"å¯ç”¨çš„æƒ…æ„Ÿæ ‡ç­¾: {', '.join(sentiments)}")

            # ç”¨æˆ·è¾“å…¥æƒ…æ„Ÿç­›é€‰
            selected = input("è¾“å…¥è¦åŒ…å«çš„æƒ…æ„Ÿ(å¤šä¸ªç”¨é€—å·åˆ†éš”, ç•™ç©ºåˆ™åŒ…å«æ‰€æœ‰): ")
            if selected:
                selected_sentiments = [s.strip() for s in selected.split(',')]
                df = df[df['sentiment'].isin(selected_sentiments)]
                print(f"ğŸ˜Š æƒ…æ„Ÿç­›é€‰: {selected_sentiments}, å‰©ä½™ {len(df)} æ¡è¯„è®º")
        # ==== ç»“æŸç­›é€‰ ====

        # ç”Ÿæˆè¯äº‘
        if not df.empty:
            generate_wordcloud(df)
        else:
            print("âš ï¸ ç­›é€‰åæ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆè¯äº‘")