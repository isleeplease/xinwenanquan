import pandas as pd
import re
import numpy as np

# è¯»å–æ•°æ®
df = pd.read_excel('éè‹±æ–‡è¯„è®ºåç»­å¤„ç†.xlsx')


# å®šä¹‰ä¼˜åŒ–ç‰ˆå¤šè¯­ç§æƒ…æ„Ÿåˆ†æè§„åˆ™
def optimized_multilingual_sentiment_analysis(text, language, cleaned_text):
    """
    ä¼˜åŒ–ç‰ˆå¤šè¯­ç§æƒ…æ„Ÿåˆ†æå‡½æ•°
    """
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å°å†™å¤„ç†
    text_str = str(text) if pd.notna(text) else ""
    text_lower = text_str.lower()
    cleaned_str = str(cleaned_text) if pd.notna(cleaned_text) else ""
    cleaned_lower = cleaned_str.lower()

    # 1. é«˜ç½®ä¿¡åº¦è§„åˆ™ - YouTubeç‰¹æœ‰çš„"W"è¡¨è¾¾è§„åˆ™ï¼ˆè·¨è¯­è¨€é€‚ç”¨ï¼‰
    w_patterns = [
        r'\bw\b', r'w stream', r'w china', r'w speed', r'w end',
        r'w guy', r'w man', r'w con', r'w gg', r'w lets go', r'w million',
        r'congrat', r'congrats', r'gg wp', r'good job', r'well done'
    ]
    if any(re.search(pattern, cleaned_lower, re.IGNORECASE) for pattern in w_patterns):
        return 'positive', 'high'

    # 2. é«˜ç½®ä¿¡åº¦è§„åˆ™ - ç§¯æè¡¨æƒ…ç¬¦å·è§„åˆ™ï¼ˆè·¨è¯­è¨€é€‚ç”¨ï¼‰
    positive_emoticons = [
        r'â¤', r'ğŸ’š', r'ğŸ’™', r'ğŸ’œ', r'ğŸ’›', r'ğŸ§¡', r'ğŸ¤', r'ğŸ–¤', r'ğŸ¤',
        r'ğŸ‰', r'ğŸŠ', r'ğŸ¥³', r'ğŸ˜', r'ğŸ˜Š', r'ğŸ˜€', r'ğŸ˜ƒ', r'ğŸ˜„', r'ğŸ˜',
        r'ğŸ˜†', r'ğŸ˜…', r'ğŸ˜‚', r'ğŸ¤£', r'ğŸ¥°', r'ğŸ˜˜', r'ğŸ˜—', r'ğŸ˜™', r'ğŸ˜š',
        r'ğŸ˜‹', r'ğŸ˜›', r'ğŸ˜', r'ğŸ˜œ', r'ğŸ¤ª', r'ğŸ¤©', r'ğŸ¤—', r'ğŸ¤ ',
        r'ğŸ’ª', r'ğŸ‘', r'ğŸ‘', r'ğŸ™Œ', r'ğŸ’¯', r'ğŸ”¥', r'âœ¨', r'ğŸŒŸ', r'â­',
        r'ğŸ’«', r'ğŸ’¥', r'ğŸ¯', r'ğŸ†', r'ğŸ¥‡', r'ğŸ¥ˆ', r'ğŸ¥‰', r'ğŸ…'
    ]
    if any(re.search(pattern, text_str) for pattern in positive_emoticons):
        return 'positive', 'high'

    # 3. é«˜ç½®ä¿¡åº¦è§„åˆ™ - æ¶ˆæè¡¨æƒ…ç¬¦å·è§„åˆ™ï¼ˆè·¨è¯­è¨€é€‚ç”¨ï¼‰
    negative_emoticons = [
        r'ğŸ’”', r'ğŸ˜­', r'ğŸ˜¢', r'ğŸ˜', r'ğŸ˜”', r'ğŸ˜Ÿ', r'ğŸ˜•', r'ğŸ™', r'â˜¹ï¸',
        r'ğŸ˜£', r'ğŸ˜–', r'ğŸ˜«', r'ğŸ˜©', r'ğŸ¥º', r'ğŸ˜¦', r'ğŸ˜§', r'ğŸ˜¨', r'ğŸ˜°',
        r'ğŸ˜¥', r'ğŸ˜“', r'ğŸ˜±', r'ğŸ˜¡', r'ğŸ˜ ', r'ğŸ¤¬', r'ğŸ˜¤', r'ğŸ¤®',
        r'ğŸ¤¢', r'ğŸ‘¿', r'ğŸ’€', r'ğŸ’©', r'ğŸ¤¡', r'ğŸ‘¹', r'ğŸ‘º', r'ğŸ‘»', r'ğŸ‘½',
        r'ğŸ‘¾', r'ğŸ¤–', r'ğŸ’£', r'ğŸ”¥', r'ğŸ–•', r'ğŸ‘'
    ]
    if any(re.search(pattern, text_str) for pattern in negative_emoticons):
        return 'negative', 'high'

    # 4. ä¸­ç­‰ç½®ä¿¡åº¦è§„åˆ™ - è‹±è¯­å…³é”®è¯è§„åˆ™ï¼ˆè·¨è¯­è¨€é€‚ç”¨ï¼‰
    english_positive_keywords = [
        'love', 'great', 'amazing', 'awesome', 'cool', 'nice', 'good',
        'best', 'perfect', 'fantastic', 'excellent', 'brilliant', 'wonderful',
        'congratulations', 'congrats', 'happy', 'fun', 'enjoy', 'like',
        'beautiful', 'gorgeous', 'fabulous', 'incredible', 'outstanding',
        'win', 'wins', 'won', 'victory', 'success', 'successful',
        'yes', 'yeah', 'yay', 'hurray', 'omg', 'wow'
    ]

    english_negative_keywords = [
        'hate', 'bad', 'terrible', 'awful', 'worst', 'horrible', 'disgusting',
        'stupid', 'idiot', 'dumb', 'fool', 'angry', 'mad', 'suck', 'sucks',
        'ridiculous', 'annoying', 'boring', 'fake', 'liar', 'scam',
        'lose', 'lost', 'failure', 'fail', 'dead', 'die', 'kill',
        'no', 'nooo', 'wtf', 'omg'
    ]

    pos_keyword_match = any(keyword in text_lower for keyword in english_positive_keywords)
    neg_keyword_match = any(keyword in text_lower for keyword in english_negative_keywords)

    if pos_keyword_match and not neg_keyword_match:
        return 'positive', 'medium'
    elif neg_keyword_match and not pos_keyword_match:
        return 'negative', 'medium'

    # 5. ä¸­ç­‰ç½®ä¿¡åº¦è§„åˆ™ - ç‰¹å®šè¯­è¨€å…³é”®è¯è§„åˆ™
    confidence_level = 'medium'

    # éŸ©è¯­å…³é”®è¯
    if language == 'ko':
        korean_positive_keywords = ['ì¢‹ì•„', 'ì¢‹ë‹¤', 'ë©‹ì§€ë‹¤', 'ìµœê³ ', 'ì§±', 'ì‚¬ë‘', 'í–‰ë³µ', 'ê¸°ë»', 'ì¢‹ë„¤ìš”', 'ëŒ€ë°•']
        korean_negative_keywords = ['ì‹«ì–´', 'ë‚˜ë¹ ', 'ë¯¸ì›Œ', 'í™”ë‚˜', 'ì§œì¦', 'ë¹¡ì³', 'ì‹«ì–´ìš”', 'ë³‘ì‹ ']

        if any(keyword in text_str for keyword in korean_positive_keywords):
            return 'positive', confidence_level
        if any(keyword in text_str for keyword in korean_negative_keywords):
            return 'negative', confidence_level

    # è¶Šå—è¯­å…³é”®è¯
    if language == 'vi':
        vietnamese_positive_keywords = ['yÃªu', 'thÃ­ch', 'tuyá»‡t', 'tá»‘t', 'Ä‘áº¹p', 'vui', 'háº¡nh phÃºc', 'tuyá»‡t vá»i',
                                        'tuyá»‡t zá»i']
        vietnamese_negative_keywords = ['ghÃ©t', 'tá»‡', 'xáº¥u', 'buá»“n', 'giáº­n', 'ghÃª tá»Ÿm', 'tá»©c giáº­n', 'Ä‘iÃªn']

        if any(keyword in text_str for keyword in vietnamese_positive_keywords):
            return 'positive', confidence_level
        if any(keyword in text_str for keyword in vietnamese_negative_keywords):
            return 'negative', confidence_level

    # æ³•è¯­å…³é”®è¯
    if language == 'fr':
        french_positive_keywords = ['aimer', 'bien', 'super', 'bon', 'beau', 'heureux', 'parfait', 'magnifique',
                                    'gÃ©nial']
        french_negative_keywords = ['dÃ©tester', 'mal', 'terrible', 'mauvais', 'triste', 'Ã©nervÃ©', 'horrible', 'nul']

        if any(keyword in text_str for keyword in french_positive_keywords):
            return 'positive', confidence_level
        if any(keyword in text_str for keyword in french_negative_keywords):
            return 'negative', confidence_level

    # å¾·è¯­å…³é”®è¯
    if language == 'de':
        german_positive_keywords = ['lieben', 'gut', 'super', 'schÃ¶n', 'perfekt', 'glÃ¼cklich', 'wunderbar',
                                    'fantastisch']
        german_negative_keywords = ['hassen', 'schlecht', 'schrecklich', 'traurig', 'Ã¤rgerlich', 'schlimm', 'schrott']

        if any(keyword in text_str for keyword in german_positive_keywords):
            return 'positive', confidence_level
        if any(keyword in text_str for keyword in german_negative_keywords):
            return 'negative', confidence_level

    # è¥¿ç­ç‰™è¯­å…³é”®è¯
    if language == 'es':
        spanish_positive_keywords = ['amar', 'bueno', 'genial', 'hermoso', 'feliz', 'perfecto', 'maravilloso',
                                     'increÃ­ble']
        spanish_negative_keywords = ['odiar', 'malo', 'terrible', 'triste', 'enojado', 'horrible', 'feo', 'estÃºpido']

        if any(keyword in text_str for keyword in spanish_positive_keywords):
            return 'positive', confidence_level
        if any(keyword in text_str for keyword in spanish_negative_keywords):
            return 'negative', confidence_level

    # 6. ä¸­ç­‰ç½®ä¿¡åº¦è§„åˆ™ - æ•°å­—å’Œç™¾åˆ†æ¯”ç›¸å…³çš„ç§¯æè¡¨è¾¾
    if re.search(r'\d+%.*(?:stream|speed|china)', text_lower):
        return 'neutral', 'medium'  # ç™¾åˆ†æ¯”è¡¨è¾¾é€šå¸¸æ˜¯ä¸­æ€§

    # 7. ä¸­ç­‰ç½®ä¿¡åº¦è§„åˆ™ - ç‰¹æ®Šæ•°å­—è¡¨è¾¾ï¼ˆå¦‚ç™¾ä¸‡è®¢é˜…ï¼‰
    if re.search(r'\d+\s*(?:million|mio|mill|Ğ¼Ğ»Ğ½|millione|millones)', text_lower):
        return 'positive', 'medium'  # æåŠç™¾ä¸‡çº§æ•°å­—é€šå¸¸æ˜¯ç§¯æçš„

    # 8. ä¸­ç­‰ç½®ä¿¡åº¦è§„åˆ™ - ç‰¹æ®Šç§¯æè¡¨è¾¾
    special_positive_patterns = [
        r'\d+\s*million', r'\d+\s*m', r'\d+\s*suscriber',
        r'lets go', r'let\'?s go', r'go go', r'yay', r'hurray'
    ]
    if any(re.search(pattern, text_lower) for pattern in special_positive_patterns):
        return 'positive', 'medium'

    # 9. ä½ç½®ä¿¡åº¦è§„åˆ™ - é»˜è®¤è¿”å›ä¸­æ€§
    return 'neutral', 'low'


# åº”ç”¨ä¼˜åŒ–ç‰ˆå¤šè¯­ç§æƒ…æ„Ÿåˆ†æ
print("æ­£åœ¨è¿›è¡Œä¼˜åŒ–ç‰ˆå¤šè¯­ç§æƒ…æ„Ÿåˆ†æ...")
results = df.apply(
    lambda row: optimized_multilingual_sentiment_analysis(row['text'], row['language'], row['cleaned_text']),
    axis=1
)

# åˆ†ç¦»æƒ…æ„Ÿæ ‡ç­¾å’Œç½®ä¿¡åº¦
df['optimized_auto_sentiment'] = [result[0] for result in results]
df['confidence_level'] = [result[1] for result in results]

# ç»Ÿè®¡æ€»ä½“æƒ…æ„Ÿåˆ†å¸ƒ
print("\nä¼˜åŒ–ç‰ˆè‡ªåŠ¨æ ‡æ³¨æƒ…æ„Ÿåˆ†å¸ƒ:")
overall_sentiment = df['optimized_auto_sentiment'].value_counts()
print(overall_sentiment)

# æŒ‰ç½®ä¿¡åº¦ç»Ÿè®¡
print("\næŒ‰ç½®ä¿¡åº¦çº§åˆ«ç»Ÿè®¡:")
confidence_stats = df.groupby(['confidence_level', 'optimized_auto_sentiment']).size().unstack(fill_value=0)
print(confidence_stats)

# è®¡ç®—é«˜ç½®ä¿¡åº¦æ ‡æ³¨æ¯”ä¾‹
high_confidence = df[df['confidence_level'] == 'high']
medium_confidence = df[df['confidence_level'] == 'medium']
low_confidence = df[df['confidence_level'] == 'low']

total_count = len(df)
high_count = len(high_confidence)
medium_count = len(medium_confidence)
low_count = len(low_confidence)

print(f"\nç½®ä¿¡åº¦åˆ†å¸ƒ:")
print(f"é«˜ç½®ä¿¡åº¦æ ‡æ³¨: {high_count} ({high_count / total_count * 100:.1f}%)")
print(f"ä¸­ç­‰ç½®ä¿¡åº¦æ ‡æ³¨: {medium_count} ({medium_count / total_count * 100:.1f}%)")
print(f"ä½ç½®ä¿¡åº¦æ ‡æ³¨: {low_count} ({low_count / total_count * 100:.1f}%)")
print(f"æ€»è®¡: {total_count} (100.0%)")

# ä¿å­˜å¸¦æœ‰ä¼˜åŒ–ç‰ˆè‡ªåŠ¨æ ‡æ³¨ç»“æœçš„æ–‡ä»¶
output_filename = 'ä¼˜åŒ–ç‰ˆå¤šè¯­ç§è‡ªåŠ¨æ ‡æ³¨ç»“æœ.xlsx'
df.to_excel(output_filename, index=False)
print(f"\nå·²ä¿å­˜å¸¦æœ‰ä¼˜åŒ–ç‰ˆè‡ªåŠ¨æ ‡æ³¨ç»“æœçš„æ–‡ä»¶: {output_filename}")

# æ˜¾ç¤ºå„ç±»åˆ«ç¤ºä¾‹
print("\næ ‡æ³¨ç¤ºä¾‹ (æŒ‰ç½®ä¿¡åº¦å’Œæƒ…æ„Ÿåˆ†ç±»):")

for confidence in ['high', 'medium']:
    print(f"\n{confidence.upper()} ç½®ä¿¡åº¦ç¤ºä¾‹:")
    for sentiment in ['positive', 'negative']:
        print(f"  {sentiment.upper()} æ ‡æ³¨:")
        sample = df[(df['confidence_level'] == confidence) & (df['optimized_auto_sentiment'] == sentiment)].head(2)
        for idx, row in sample.iterrows():
            print(f"    è¯­è¨€: {row['language']} | è¯„è®º: {row['text']}")
            print(f"    æ ‡æ³¨: {row['optimized_auto_sentiment']} | ç½®ä¿¡åº¦: {row['confidence_level']}")
            print("    ---")